{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ae6874-7585-4a0b-848c-965f639def41",
   "metadata": {},
   "source": [
    "# Text Embedding\n",
    "In this notebook, we generate 10-K filings text embeddings with the Vertex AI [textembedding-gecko](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings) model.  Unstructured text from 10-K filings has been extracted using a parser beforehand.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Get 10-K filings unstructured text from a Google storage bucket\n",
    "2. specifically select Item 1 from the 10K which describes the business of the company: who and what the company does, what subsidiaries it owns, and what markets it operates in. \n",
    "3. Chunk the text into natural sections using NLTK (to avoid input token limits)\n",
    "4. Save text with embeddings to csv to stage for loading into graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de69a3c-2817-4ab6-9c02-a2cf500feb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user tabulate sentence-transformers\n",
    "%pip install --user altair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386daec-95ed-40b9-8222-efa65197f426",
   "metadata": {},
   "source": [
    "Be sure to restart the kernel after you run the pip command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b0998-b1f5-43f6-a278-9242e1b2b71e",
   "metadata": {},
   "source": [
    "## Get 10-K Filings from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4286ddb9-950d-4bee-ba7b-c717e3ee1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "storage_client.bucket('neo4j-datasets').blob('hands-on-lab/form10k.zip').download_to_filename('/home/jupyter/form10k.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09449433-e3b9-4946-84ba-05a1ab0c47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/jupyter/form10k\n",
    "!unzip -qq -n '/home/jupyter/form10k.zip' -d /home/jupyter/form10k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dbe14-2aaa-4df0-97cc-6e03d47dd655",
   "metadata": {},
   "source": [
    "## 10-K Filings Exploration and Chunking\n",
    "Let's open one file to understand its contents.  It is actually a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d86432-2679-4f0a-a33c-c16064769d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/jupyter/form10k/0001830197-22-000038.txt') as f:\n",
    "    f10_k = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f8c51-b2fc-4704-85c0-12cd3b5a8c1f",
   "metadata": {},
   "source": [
    "We are interested in Item 1 specifically. \n",
    "\n",
    "Item 1 describes the business of the company: who and what the company does, what subsidiaries it owns, and what markets it operates in. It may also include recent events, competition, regulations, and labor issues. (Some industries are heavily regulated, and have complex labor requirements, which have significant effects on the business.) Other topics in this section may include special operating costs, seasonal factors, or insurance matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76566ea2-d2da-4dae-b196-d66c9570c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f10_k['item1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63be173-75e7-4017-9d61-8fe683ea199e",
   "metadata": {},
   "source": [
    "This text has the ability to exceed token limits for `textembedding-gecko`.  Also the quality of embeddings can go down if the text gets to large. As such we should find some way to chunk the text up into seperate sections for embedding.\n",
    "\n",
    "Below is a way to do this with Langchain's `RecursiveCharacterTextSplitter` which takes into account of Chunk overlaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04061873-2b51-40b2-98ea-8cfd46e6fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = f10_k['item1']\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 15,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "docs = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d48aca-8acd-4ea2-ae2b-e8e0e18f2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Item 1. Business \n",
      "Company Overview\n",
      "We are a leading residential mortgage originator and servicer driven by a mission to create financially healthy, happy homeowners. We do this by delivering scale, efficiency and savings to our partners and customers. Our business model is focused on leveraging a nationwide network of partner relationships to drive sustainable origination growth. We support our origination operations through a robust operational infrastructure and a highly responsive customer experience. We then leverage our servicing platform to manage the customer experience. We believe that the complementary relationship between our origination and servicing businesses allows us to provide a best-in-class experience to our customers throughout their homeownership lifecycle.\n",
      "Our primary focus is our Wholesale channel, which is a business-to-business-to-customer distribution model in which we utilize our relationships with independent mortgage brokerages, which we refer to as our Broker Partners, to reach our end-borrower customers. In this channel, while our Broker Partners establish and maintain the relationship with the end-borrower, we as the lender underwrite the loan in-house and act as the original lender. This differentiates our Wholesale channel from our other two channels of mortgage origination: in our Direct channel, we as the lender engage with the end-borrower customers directly to originate mortgages, and in our Correspondent channel, we as the lender engage with original lenders, which we refer to as our Correspondent Partners, to purchase loans already issued to end-borrower customers.\n",
      "According to \n",
      "Inside Mortgage Finance\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede16b2-a881-48b5-b8ea-f3e4de6f0022",
   "metadata": {},
   "source": [
    "## Get 10-K Text Embeddings with Vertex AI\n",
    "Now that we understand our data and how to chunk it.  Let's Generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e8ff3f6-fba3-4d86-b153-a637575fdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from typing import List\n",
    "\n",
    "EMBEDDING_MODEL = TextEmbeddingModel\n",
    "\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            # print(f'Sleeping {sleep_time:.1f} seconds')\n",
    "            time.sleep(sleep_time)\n",
    "                \n",
    "def embed_documents(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Call Vertex LLM embedding endpoint for embedding docs\n",
    "    Args:\n",
    "    texts: The list of texts to embed.\n",
    "    Returns:\n",
    "    List of embeddings, one for each text.\n",
    "    \"\"\"\n",
    "    model = EMBEDDING_MODEL.from_pretrained(\"textembedding-gecko@001\")\n",
    "\n",
    "    limiter = rate_limit(600)\n",
    "    results = []\n",
    "    docs = list(texts)\n",
    "\n",
    "    while docs:\n",
    "        # Working in batches of 2 because the API apparently won't let\n",
    "        # us send more than 2 documents per request to get embeddings.\n",
    "        head, docs = docs[:2], docs[2:]\n",
    "        # print(f'Sending embedding request for: {head!r}')\n",
    "        chunk = model.get_embeddings(head)\n",
    "        results.extend(chunk)\n",
    "        next(limiter)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e56acd5-e781-4ce7-b25f-49abdd9de8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need a chunking utility to stay within token limits as we loop through files\n",
    "def chunks(xs, n=3):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee20c6ac-6793-4de6-aaac-f83bb2178d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_text_embedding_entries(input_text:str, company_name: str, cusip: str):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 2000,\n",
    "        chunk_overlap  = 15,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False,\n",
    "    )\n",
    "    docs = text_splitter.split_text(input_text)\n",
    "    res = []\n",
    "    seq_id = -1\n",
    "    for d in chunks(docs):\n",
    "        embeddings = embed_documents(d)\n",
    "        # throttle so we don't blow through the quota.\n",
    "        # time.sleep(1)\n",
    "        \n",
    "        for i in range(len(d)):\n",
    "            seq_id += 1\n",
    "            res.append({'companyName': company_name, 'cusip': cusip, 'seqId': seq_id, 'contextId': company_name + str(seq_id), 'textEmbedding': embeddings[i].values, 'text': d[i]})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f3006c7-fa97-4282-bb31-6352ab0ed6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_names = os.listdir('/home/jupyter/form10k/')\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cfb362-264d-412e-a6c6-0e372fde8943",
   "metadata": {},
   "source": [
    "This cell takes about 15 minutes to run.  That's largely down to us throttling so we don't exceed the quota on our free account.  If you're using an enterprise account, you won't need to throttle like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8497e11-d4f5-4bb3-80de-8cc538bd16ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 5 of 95\n",
      "Parsed 10 of 95\n",
      "Parsed 15 of 95\n",
      "Parsed 20 of 95\n",
      "Parsed 25 of 95\n",
      "Parsed 30 of 95\n",
      "Parsed 35 of 95\n",
      "Parsed 40 of 95\n",
      "Parsed 45 of 95\n",
      "Parsed 50 of 95\n",
      "Parsed 55 of 95\n",
      "Parsed 60 of 95\n",
      "Parsed 65 of 95\n",
      "Parsed 70 of 95\n",
      "Parsed 75 of 95\n",
      "Parsed 80 of 95\n",
      "Parsed 85 of 95\n",
      "Parsed 90 of 95\n",
      "CPU times: user 32.1 s, sys: 6.19 s, total: 38.3 s\n",
      "Wall time: 4min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3652"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "# We're hitting the quota, so we're going to sleep for a bit to zero it out for sure, then throttle our calls\n",
    "# time.sleep(60)\n",
    "\n",
    "count = 0\n",
    "embedding_entries = []\n",
    "for file_name in file_names:\n",
    "    if '.txt' in file_name:\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(f'Parsed {count} of {len(file_names)}')\n",
    "        with open('/home/jupyter/form10k/' + file_name) as f:\n",
    "            f10_k = json.load(f)\n",
    "        embedding_entries.extend(create_text_embedding_entries(f10_k['item1'], f10_k['companyName'], f10_k['cusip']))\n",
    "len(embedding_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446e651-9134-4634-a527-4f2745c7d623",
   "metadata": {},
   "source": [
    "## Save 10-K Documents with Embeddings\n",
    "We will save these locally to use in graph loading, in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54ca3c83-fdc8-4d30-bacd-4c0b0328655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "edf = pd.DataFrame(embedding_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "971cd795-4097-415f-83d6-f4caa651218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyName</th>\n",
       "      <th>cusip</th>\n",
       "      <th>seqId</th>\n",
       "      <th>contextId</th>\n",
       "      <th>textEmbedding</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DOLLAR TREE STORES I</td>\n",
       "      <td>256677105</td>\n",
       "      <td>0</td>\n",
       "      <td>DOLLAR TREE STORES I0</td>\n",
       "      <td>[0.0011005396954715252, -0.02110779657959938, ...</td>\n",
       "      <td>&gt;Item 1. Business\\n” for further discussion of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DOLLAR TREE STORES I</td>\n",
       "      <td>256677105</td>\n",
       "      <td>1</td>\n",
       "      <td>DOLLAR TREE STORES I1</td>\n",
       "      <td>[0.006865902338176966, 0.003632724517956376, 0...</td>\n",
       "      <td>Plus\\n;\\n•\\nthe introduction of selected Dolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DOLLAR TREE STORES I</td>\n",
       "      <td>256677105</td>\n",
       "      <td>2</td>\n",
       "      <td>DOLLAR TREE STORES I2</td>\n",
       "      <td>[-0.04646346718072891, -0.009276329539716244, ...</td>\n",
       "      <td>The rollout of our initiative to add price poi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DOLLAR TREE STORES I</td>\n",
       "      <td>256677105</td>\n",
       "      <td>3</td>\n",
       "      <td>DOLLAR TREE STORES I3</td>\n",
       "      <td>[-0.007591314613819122, -0.01885572448372841, ...</td>\n",
       "      <td>In fiscal 2019, we recorded a $313.0 million n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DOLLAR TREE STORES I</td>\n",
       "      <td>256677105</td>\n",
       "      <td>4</td>\n",
       "      <td>DOLLAR TREE STORES I4</td>\n",
       "      <td>[-0.008723229169845581, -0.014396817423403263,...</td>\n",
       "      <td>We rely extensively on our computer and techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>11</td>\n",
       "      <td>ARK RESTAURANTS CORP11</td>\n",
       "      <td>[-0.034189801663160324, -0.022611897438764572,...</td>\n",
       "      <td>We have experienced aggressive competition for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>12</td>\n",
       "      <td>ARK RESTAURANTS CORP12</td>\n",
       "      <td>[-0.04284031689167023, -0.028181184083223343, ...</td>\n",
       "      <td>9\\nAlcoholic beverage control regulations requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>13</td>\n",
       "      <td>ARK RESTAURANTS CORP13</td>\n",
       "      <td>[-0.03191894665360451, -0.026221390813589096, ...</td>\n",
       "      <td>We are subject to “dram-shop” statutes in most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>14</td>\n",
       "      <td>ARK RESTAURANTS CORP14</td>\n",
       "      <td>[-0.02670503407716751, -0.029166726395487785, ...</td>\n",
       "      <td>Our business is highly seasonal; however, our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>15</td>\n",
       "      <td>ARK RESTAURANTS CORP15</td>\n",
       "      <td>[-0.048986222594976425, -0.033875517547130585,...</td>\n",
       "      <td>Our principal executive offices are located at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3652 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               companyName      cusip  seqId               contextId  \\\n",
       "0     DOLLAR TREE STORES I  256677105      0   DOLLAR TREE STORES I0   \n",
       "1     DOLLAR TREE STORES I  256677105      1   DOLLAR TREE STORES I1   \n",
       "2     DOLLAR TREE STORES I  256677105      2   DOLLAR TREE STORES I2   \n",
       "3     DOLLAR TREE STORES I  256677105      3   DOLLAR TREE STORES I3   \n",
       "4     DOLLAR TREE STORES I  256677105      4   DOLLAR TREE STORES I4   \n",
       "...                    ...        ...    ...                     ...   \n",
       "3647  ARK RESTAURANTS CORP  00214Q104     11  ARK RESTAURANTS CORP11   \n",
       "3648  ARK RESTAURANTS CORP  00214Q104     12  ARK RESTAURANTS CORP12   \n",
       "3649  ARK RESTAURANTS CORP  00214Q104     13  ARK RESTAURANTS CORP13   \n",
       "3650  ARK RESTAURANTS CORP  00214Q104     14  ARK RESTAURANTS CORP14   \n",
       "3651  ARK RESTAURANTS CORP  00214Q104     15  ARK RESTAURANTS CORP15   \n",
       "\n",
       "                                          textEmbedding  \\\n",
       "0     [0.0011005396954715252, -0.02110779657959938, ...   \n",
       "1     [0.006865902338176966, 0.003632724517956376, 0...   \n",
       "2     [-0.04646346718072891, -0.009276329539716244, ...   \n",
       "3     [-0.007591314613819122, -0.01885572448372841, ...   \n",
       "4     [-0.008723229169845581, -0.014396817423403263,...   \n",
       "...                                                 ...   \n",
       "3647  [-0.034189801663160324, -0.022611897438764572,...   \n",
       "3648  [-0.04284031689167023, -0.028181184083223343, ...   \n",
       "3649  [-0.03191894665360451, -0.026221390813589096, ...   \n",
       "3650  [-0.02670503407716751, -0.029166726395487785, ...   \n",
       "3651  [-0.048986222594976425, -0.033875517547130585,...   \n",
       "\n",
       "                                                   text  \n",
       "0     >Item 1. Business\\n” for further discussion of...  \n",
       "1     Plus\\n;\\n•\\nthe introduction of selected Dolla...  \n",
       "2     The rollout of our initiative to add price poi...  \n",
       "3     In fiscal 2019, we recorded a $313.0 million n...  \n",
       "4     We rely extensively on our computer and techno...  \n",
       "...                                                 ...  \n",
       "3647  We have experienced aggressive competition for...  \n",
       "3648  9\\nAlcoholic beverage control regulations requ...  \n",
       "3649  We are subject to “dram-shop” statutes in most...  \n",
       "3650  Our business is highly seasonal; however, our ...  \n",
       "3651  Our principal executive offices are located at...  \n",
       "\n",
       "[3652 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e64c29-2139-4694-ba7f-751bbd8c9daf",
   "metadata": {},
   "source": [
    "Provide your Neo4j credentials.  We need the DB conection URL, the username (probably `neo4j`), and your password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58743d29-c611-4927-b61a-2b05721a9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# username is neo4j by default\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "\n",
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://6688b25b.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = '_kogrNk53u8oTk5be55kmit1kHGdhZj98yJlG-VYSR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "916b25a2-4dc8-454d-8fb6-06862b115ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f323e2e-7288-4887-b104-35be62dcae99",
   "metadata": {},
   "source": [
    "Remember to create indexes. We will be merging 10K documents by `companyName`. In a production setting, we would want to use a better identifier here (like we did with cusip for Company) However, this should suffice for our intents and purposes as we are just getting acquainted to learning about semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a57ef3d0-83f8-4d6d-9824-2f5206c36f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE INDEX company_name IF NOT EXISTS FOR (n:Company) ON (n.companyName)')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_document_id IF NOT EXISTS FOR (n:Document) REQUIRE (n.documentId) IS NODE KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be481f-7be8-4e70-b441-98c9d629b56b",
   "metadata": {},
   "source": [
    "Due to the size of the documents we will want to transform the dataframe into a list of dict that we can chunk up and insert via parameterized query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0e68d63-b162-457b-8a60-aec01828aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_entries = edf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eecde179-7b86-46b8-a3dd-ae75b902472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 of 3652\n",
      "loaded 200 of 3652\n",
      "loaded 300 of 3652\n",
      "loaded 400 of 3652\n",
      "loaded 500 of 3652\n",
      "loaded 600 of 3652\n",
      "loaded 700 of 3652\n",
      "loaded 800 of 3652\n",
      "loaded 900 of 3652\n",
      "loaded 1000 of 3652\n",
      "loaded 1100 of 3652\n",
      "loaded 1200 of 3652\n",
      "loaded 1300 of 3652\n",
      "loaded 1400 of 3652\n",
      "loaded 1500 of 3652\n",
      "loaded 1600 of 3652\n",
      "loaded 1700 of 3652\n",
      "loaded 1800 of 3652\n",
      "loaded 1900 of 3652\n",
      "loaded 2000 of 3652\n",
      "loaded 2100 of 3652\n",
      "loaded 2200 of 3652\n",
      "loaded 2300 of 3652\n",
      "loaded 2400 of 3652\n",
      "loaded 2500 of 3652\n",
      "loaded 2600 of 3652\n",
      "loaded 2700 of 3652\n",
      "loaded 2800 of 3652\n",
      "loaded 2900 of 3652\n",
      "loaded 3000 of 3652\n",
      "loaded 3100 of 3652\n",
      "loaded 3200 of 3652\n",
      "loaded 3300 of 3652\n",
      "loaded 3400 of 3652\n",
      "loaded 3500 of 3652\n",
      "loaded 3600 of 3652\n",
      "loaded 3652 of 3652\n"
     ]
    }
   ],
   "source": [
    "total = len(emb_entries)\n",
    "count = 0\n",
    "for d in chunks(emb_entries, 100):\n",
    "    gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH(c:Company {cusip:record.cusip})\n",
    "    MERGE(b:Document {documentId:record.contextId})\n",
    "    SET b.documentType = 'FORM_10K_ITEM1', b.seqId = record.seqId, b.textEmbedding = record.textEmbedding, b.text = record.text\n",
    "    MERGE(c)-[:HAS]->(b)\n",
    "    RETURN count(b) as cnt\n",
    "    ''', params = {'records':d})\n",
    "    count += len(d)\n",
    "    print(f'loaded {count} of {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75200e1-d4d3-4aaf-aae5-b989a99a855c",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45754ddd-c62c-4d4f-9e80-0648e99933db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(doc)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(doc)\n",
       "0        3536"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check node count\n",
    "gds.run_cypher('MATCH(doc:Document) RETURN count(doc)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf4496-6783-4ec3-be91-2dad92bc477b",
   "metadata": {},
   "source": [
    "Note that we were only getting 10-K docs for a minority of companies. It should be fine for this, but in a more rigorous setting, you may want to try and pull more.  There are likely a few factors attributing to this. \n",
    "\n",
    "1. We used company names to search EDGAR which resulted in many misses and dups which were discarded. In a more rigorous setting, we would investigate other endpoints and use more parsing to extract EDGAR cik keys for exact matching companies when pulling forms.\n",
    "\n",
    "2. Company names are not consistent across form13 filings, so even if we successfully pull on one version of a company name, we may not be able to merge it into the graph via the one company name represented there. \n",
    "\n",
    "3. Not all companies in the dataset are obligated to file 10-Ks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1034a8eb-cd6d-4a09-913b-d1d5e97c1bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>numWithDocs</th>\n",
       "      <th>PercWithDocs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242</td>\n",
       "      <td>48</td>\n",
       "      <td>19.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total  numWithDocs  PercWithDocs\n",
       "0    242           48         19.83"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check count and percentage of companies with 10-K docs.  Note it is the minority\n",
    "gds.run_cypher('''\n",
    "MATCH(b:Company)\n",
    "WITH b, count{(b)-[:HAS]->(d:Document)} AS docCount\n",
    "WITH count(b) AS total, sum(toInteger(docCount > 0)) AS numWithDocs\n",
    "RETURN total, numWithDocs, round(100*toFloat(numWithDocs)/toFloat(total), 2) As PercWithDocs\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9c940-40cd-4fe9-8278-ef064e43933e",
   "metadata": {},
   "source": [
    "You might note that there are duplicate names.  For our purposes here, we will treat it as entity resolution, meaning that we treat companies with the same name as belonging to the same overarching entity for semantic search. In a more rigorous setting, we would need to disambiguate with other EDGAR keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da6b2835-2dfe-42ff-854f-c639eee67672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalCompanies</th>\n",
       "      <th>uniqueCompanyNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   totalCompanies  uniqueCompanyNames\n",
       "0             242                 241"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show duplicates via HAS relationship\n",
    "gds.run_cypher('''\n",
    "MATCH(b:Company)\n",
    "RETURN count(b) AS totalCompanies, count(DISTINCT b.companyName) AS uniqueCompanyNames\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64854d9c-1776-451b-a3cb-60414d8116e3",
   "metadata": {},
   "source": [
    "## View Embeddings as Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb9e08-bbaf-494c-bf47-d9dc49944300",
   "metadata": {},
   "source": [
    "Vector embeddings generated by language models are nothing but numerical representation of words or sentences.  So, similar sentences will be located nearby.  The embeddings we generated earlier are higher dimensional ones.  To visualize them, we need to reduce the dimensionality.  Let's do that and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f690e19-1d53-47fe-974c-9fda3c5903ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def generate_chart(df, xcol, ycol, lbl = 'on', color = 'basic', title = '', tooltips = ['documentId'], label = ''):\n",
    "  chart = alt.Chart(df).mark_circle(size=30).encode(\n",
    "    x = alt.X(xcol,\n",
    "        scale=alt.Scale(zero = False),\n",
    "        axis=alt.Axis(labels = False, ticks = False, domain = False)\n",
    "    ),\n",
    "    y = alt.Y(ycol,\n",
    "        scale=alt.Scale(zero = False),\n",
    "        axis=alt.Axis(labels = False, ticks = False, domain = False)\n",
    "    ),\n",
    "    color= alt.value('#333293') if color == 'basic' else color,\n",
    "    tooltip=tooltips\n",
    "    )\n",
    "\n",
    "  if lbl == 'on':\n",
    "    text = chart.mark_text(align = 'left', baseline = 'middle', dx = 7, size = 5, color = 'black').encode(text = label, color = alt.value('black'))\n",
    "  else:\n",
    "    text = chart.mark_text(align = 'left', baseline = 'middle', dx = 10).encode()\n",
    "\n",
    "  result = (chart + text).configure(background=\"#FDF7F0\"\n",
    "        ).properties(\n",
    "        width = 800,\n",
    "        height = 500,\n",
    "        title = title\n",
    "       ).configure_legend(\n",
    "  orient = 'bottom', titleFontSize = 18, labelFontSize = 18)\n",
    "        \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e875832-4e4c-47d3-a535-e0b5fb177284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to return the principal components\n",
    "def get_pc(arr, n):\n",
    "  pca = PCA(n_components = n)\n",
    "  embeds_transform = pca.fit_transform(arr)\n",
    "  return embeds_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3aa76707-4150-4948-83b0-d454f086afe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyName</th>\n",
       "      <th>documentId</th>\n",
       "      <th>text</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>AMAZON C2</td>\n",
       "      <td>We serve authors and independent publishers wi...</td>\n",
       "      <td>[0.006612538825720549, -0.011562603525817394, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>AMAZON C1</td>\n",
       "      <td>Consumers\\nWe serve consumers through our onli...</td>\n",
       "      <td>[-0.0006532742991112173, -0.01848229579627514,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>AMAZON C3</td>\n",
       "      <td>Our businesses encompass a large variety of pr...</td>\n",
       "      <td>[-0.0012651049764826894, -0.03241520747542381,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>AMAZON C8</td>\n",
       "      <td>70\\nCo-CEO, President, and Chair of IronNet Cy...</td>\n",
       "      <td>[0.007523578125983477, -0.05875156447291374, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>AMAZON C0</td>\n",
       "      <td>&gt;Item 1.\\nBusiness\\nThis Annual Report on Form...</td>\n",
       "      <td>[0.023851962760090828, -0.020312661305069923, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>GLOBAL X CLOUD COMPUTING ETF</td>\n",
       "      <td>Global Partner Acquisition Corp Ii21</td>\n",
       "      <td>We are not prohibited from pursuing an initial...</td>\n",
       "      <td>[0.0037235927302390337, -0.027355222031474113,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>GLOBAL X CLOUD COMPUTING ETF</td>\n",
       "      <td>Global Partner Acquisition Corp Ii72</td>\n",
       "      <td>➤\\n\\n\\nIf we seek shareholder approval of our ...</td>\n",
       "      <td>[0.014228932559490204, -0.0016051246784627438,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>GLOBAL X CLOUD COMPUTING ETF</td>\n",
       "      <td>Global Partner Acquisition Corp Ii48</td>\n",
       "      <td>However, we would not be restricting our share...</td>\n",
       "      <td>[0.01272459514439106, -0.021930834278464317, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>GLOBAL X CLOUD COMPUTING ETF</td>\n",
       "      <td>Global Partner Acquisition Corp Ii50</td>\n",
       "      <td>The foregoing is different from the procedures...</td>\n",
       "      <td>[0.023618966341018677, -0.018364861607551575, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>GLOBAL X CLOUD COMPUTING ETF</td>\n",
       "      <td>Global Partner Acquisition Corp Ii58</td>\n",
       "      <td>the trust account for any reason. In order to ...</td>\n",
       "      <td>[-0.0017916089855134487, -0.024078311398625374...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      companyName                            documentId  \\\n",
       "0                  Amazon.com Inc                             AMAZON C2   \n",
       "1                  Amazon.com Inc                             AMAZON C1   \n",
       "2                  Amazon.com Inc                             AMAZON C3   \n",
       "3                  Amazon.com Inc                             AMAZON C8   \n",
       "4                  Amazon.com Inc                             AMAZON C0   \n",
       "..                            ...                                   ...   \n",
       "995  GLOBAL X CLOUD COMPUTING ETF  Global Partner Acquisition Corp Ii21   \n",
       "996  GLOBAL X CLOUD COMPUTING ETF  Global Partner Acquisition Corp Ii72   \n",
       "997  GLOBAL X CLOUD COMPUTING ETF  Global Partner Acquisition Corp Ii48   \n",
       "998  GLOBAL X CLOUD COMPUTING ETF  Global Partner Acquisition Corp Ii50   \n",
       "999  GLOBAL X CLOUD COMPUTING ETF  Global Partner Acquisition Corp Ii58   \n",
       "\n",
       "                                                  text  \\\n",
       "0    We serve authors and independent publishers wi...   \n",
       "1    Consumers\\nWe serve consumers through our onli...   \n",
       "2    Our businesses encompass a large variety of pr...   \n",
       "3    70\\nCo-CEO, President, and Chair of IronNet Cy...   \n",
       "4    >Item 1.\\nBusiness\\nThis Annual Report on Form...   \n",
       "..                                                 ...   \n",
       "995  We are not prohibited from pursuing an initial...   \n",
       "996  ➤\\n\\n\\nIf we seek shareholder approval of our ...   \n",
       "997  However, we would not be restricting our share...   \n",
       "998  The foregoing is different from the procedures...   \n",
       "999  the trust account for any reason. In order to ...   \n",
       "\n",
       "                                                   emb  \n",
       "0    [0.006612538825720549, -0.011562603525817394, ...  \n",
       "1    [-0.0006532742991112173, -0.01848229579627514,...  \n",
       "2    [-0.0012651049764826894, -0.03241520747542381,...  \n",
       "3    [0.007523578125983477, -0.05875156447291374, -...  \n",
       "4    [0.023851962760090828, -0.020312661305069923, ...  \n",
       "..                                                 ...  \n",
       "995  [0.0037235927302390337, -0.027355222031474113,...  \n",
       "996  [0.014228932559490204, -0.0016051246784627438,...  \n",
       "997  [0.01272459514439106, -0.021930834278464317, -...  \n",
       "998  [0.023618966341018677, -0.018364861607551575, ...  \n",
       "999  [-0.0017916089855134487, -0.024078311398625374...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = gds.run_cypher(\"MATCH (c:Company)-[:HAS]->(n:Document) RETURN c.companyName as companyName, n.documentId as documentId, n.text as text, n.textEmbedding as emb LIMIT 1000\")\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99731d73-3648-43f1-b99d-79300f7df67d",
   "metadata": {},
   "source": [
    "## K-Means Clustering on the Embeddings\n",
    "Let's run the K-Means Clustering algorithm and view similar document chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2335c8-34bc-42ee-b093-ec3c5f718a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "embeds = np.array(emb_df['emb'].tolist())\n",
    "embeds_pc2 = get_pc(embeds, 2)\n",
    "\n",
    "df_clust = pd.concat([emb_df, pd.DataFrame(embeds_pc2)], axis = 1)\n",
    "n_clusters = 5\n",
    "\n",
    "kmeans_model = KMeans(n_clusters = n_clusters, n_init = 1, random_state = 0)\n",
    "classes = kmeans_model.fit_predict(embeds).tolist()\n",
    "df_clust['cluster'] = (list(map(str,classes)))\n",
    "\n",
    "df_clust.columns = df_clust.columns.astype(str)\n",
    "generate_chart(df_clust.iloc[:],'0', '1', lbl = 'off', color = 'cluster', title = 'K-Means Clustering with n Clusters', tooltips = ['documentId', 'text'], label = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2fc11b-db00-493f-b289-6831c9960442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "lab (Local)",
   "language": "python",
   "name": "local-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
