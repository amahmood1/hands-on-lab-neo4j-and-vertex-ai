{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ae6874-7585-4a0b-848c-965f639def41",
   "metadata": {},
   "source": [
    "# Text Embedding\n",
    "In this notebook, we generate 10-K filings text embeddings with the Vertex AI [textembedding-gecko](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings) model.  Unstructured text from 10-K filings has been extracted using a parser beforehand.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Get 10-K filings unstructured text from a Google storage bucket\n",
    "2. specifically select Item 1 from the 10K which describes the business of the company: who and what the company does, what subsidiaries it owns, and what markets it operates in. \n",
    "3. Chunk the text into natural sections using NLTK (to avoid input token limits)\n",
    "4. Save text with embeddings to csv to stage for loading into graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de69a3c-2817-4ab6-9c02-a2cf500feb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (0.9.0)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for torch>=1.6.0 from https://files.pythonhosted.org/packages/6d/13/b5e8bacd980b2195f8a1741ce11cbb9146568607795d5e4ff510dcff1064/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchvision (from sentence-transformers)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/84/eb/4f6483ae9094e164dc5b9b792e377f7d37823b0bedc3eef3193d416d2bb6/torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.2)\n",
      "Collecting nltk (from sentence-transformers)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from sentence-transformers) (0.17.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Collecting sympy (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/4d/22/91a8af421c8a8902dde76e6ef3db01b258af16c53d81e8c0d0dc13900a9e/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/0a/f8/5193b57555cbeecfdb6ade643df0d4218cc6385485492b6e2f64ceae53bb/nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m950.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m915.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=85d472f749f42c1a4452fe5643c42eb8a65d70f6cb4cd512698f03c52c377b52\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, mpmath, triton, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, tokenizers, nvidia-cusolver-cu12, transformers, torch, torchvision, sentence-transformers\n",
      "\u001b[33m  WARNING: The script isympy is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed mpmath-1.3.0 nltk-3.8.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.2.140 nvidia-nvtx-cu12-12.1.105 regex-2023.10.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.14.1 torch-2.1.0 torchvision-0.16.0 transformers-4.34.0 triton-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: altair in /home/jupyter/.local/lib/python3.10/site-packages (5.1.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair) (4.19.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from altair) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from altair) (23.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from altair) (2.0.3)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from altair) (4.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->altair) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->altair) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->altair) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.25->altair) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user tabulate sentence-transformers\n",
    "%pip install --user altair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386daec-95ed-40b9-8222-efa65197f426",
   "metadata": {},
   "source": [
    "Be sure to restart the kernel after you run the pip command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b0998-b1f5-43f6-a278-9242e1b2b71e",
   "metadata": {},
   "source": [
    "## Get 10-K Filings from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4286ddb9-950d-4bee-ba7b-c717e3ee1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "storage_client.bucket('neo4j-datasets').blob('hands-on-lab/form10k.zip').download_to_filename('/home/jupyter/form10k.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09449433-e3b9-4946-84ba-05a1ab0c47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/jupyter/form10k\n",
    "!unzip -qq -n '/home/jupyter/form10k.zip' -d /home/jupyter/form10k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64dbe14-2aaa-4df0-97cc-6e03d47dd655",
   "metadata": {},
   "source": [
    "## 10-K Filings Exploration and Chunking\n",
    "Let's open one file to understand its contents.  It is actually a json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d86432-2679-4f0a-a33c-c16064769d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/jupyter/form10k/0001830197-22-000038.txt') as f:\n",
    "    f10_k = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f8c51-b2fc-4704-85c0-12cd3b5a8c1f",
   "metadata": {},
   "source": [
    "We are interested in Item 1 specifically. \n",
    "\n",
    "Item 1 describes the business of the company: who and what the company does, what subsidiaries it owns, and what markets it operates in. It may also include recent events, competition, regulations, and labor issues. (Some industries are heavily regulated, and have complex labor requirements, which have significant effects on the business.) Other topics in this section may include special operating costs, seasonal factors, or insurance matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76566ea2-d2da-4dae-b196-d66c9570c8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f10_k['item1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63be173-75e7-4017-9d61-8fe683ea199e",
   "metadata": {},
   "source": [
    "This text has the ability to exceed token limits for `textembedding-gecko`.  Also the quality of embeddings can go down if the text gets to large. As such we should find some way to chunk the text up into seperate sections for embedding.\n",
    "\n",
    "Below is a way to do this with Langchain's `RecursiveCharacterTextSplitter` which takes into account of Chunk overlaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04061873-2b51-40b2-98ea-8cfd46e6fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = f10_k['item1']\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 15,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "docs = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d48aca-8acd-4ea2-ae2b-e8e0e18f2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Item 1. Business \n",
      "Company Overview\n",
      "We are a leading residential mortgage originator and servicer driven by a mission to create financially healthy, happy homeowners. We do this by delivering scale, efficiency and savings to our partners and customers. Our business model is focused on leveraging a nationwide network of partner relationships to drive sustainable origination growth. We support our origination operations through a robust operational infrastructure and a highly responsive customer experience. We then leverage our servicing platform to manage the customer experience. We believe that the complementary relationship between our origination and servicing businesses allows us to provide a best-in-class experience to our customers throughout their homeownership lifecycle.\n",
      "Our primary focus is our Wholesale channel, which is a business-to-business-to-customer distribution model in which we utilize our relationships with independent mortgage brokerages, which we refer to as our Broker Partners, to reach our end-borrower customers. In this channel, while our Broker Partners establish and maintain the relationship with the end-borrower, we as the lender underwrite the loan in-house and act as the original lender. This differentiates our Wholesale channel from our other two channels of mortgage origination: in our Direct channel, we as the lender engage with the end-borrower customers directly to originate mortgages, and in our Correspondent channel, we as the lender engage with original lenders, which we refer to as our Correspondent Partners, to purchase loans already issued to end-borrower customers.\n",
      "According to \n",
      "Inside Mortgage Finance\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede16b2-a881-48b5-b8ea-f3e4de6f0022",
   "metadata": {},
   "source": [
    "## Get 10-K Text Embeddings with Vertex AI\n",
    "Now that we understand our data and how to chunk it.  Let's Generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e8ff3f6-fba3-4d86-b153-a637575fdbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingModel\n",
    "EMBEDDING_MODEL = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e56acd5-e781-4ce7-b25f-49abdd9de8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need a chunking utility to stay within token limits as we loop through files\n",
    "def chunks(xs, n=3):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee20c6ac-6793-4de6-aaac-f83bb2178d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_text_embedding_entries(input_text:str, company_name: str, cusip: str):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 2000,\n",
    "        chunk_overlap  = 15,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False,\n",
    "    )\n",
    "    docs = text_splitter.split_text(input_text)\n",
    "    res = []\n",
    "    seq_id = -1\n",
    "    for d in chunks(docs):\n",
    "        embeddings = EMBEDDING_MODEL.get_embeddings(d)\n",
    "        \n",
    "        # throttle so we don't blow through the quota.\n",
    "        time.sleep(1)\n",
    "        \n",
    "        for i in range(len(d)):\n",
    "            seq_id += 1\n",
    "            res.append({'companyName': company_name, 'cusip': cusip, 'seqId': seq_id, 'contextId': company_name + str(seq_id), 'textEmbedding': embeddings[i].values, 'text': d[i]})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0a9f0-d7bb-404c-b899-fd497602cebc",
   "metadata": {},
   "source": [
    "Due to Quota Limitations, lets only do 5 form 10k files out of the 95 we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f3006c7-fa97-4282-bb31-6352ab0ed6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_names = os.listdir('/home/jupyter/form10k/')[0:5]\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cfb362-264d-412e-a6c6-0e372fde8943",
   "metadata": {},
   "source": [
    "This cell takes about 15 minutes to run.  That's largely down to us throttling so we don't exceed the quota on our free account.  If you're using an enterprise account, you won't need to throttle like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8497e11-d4f5-4bb3-80de-8cc538bd16ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 5 of 5\n",
      "CPU times: user 1.06 s, sys: 68.7 ms, total: 1.13 s\n",
      "Wall time: 52.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "embedding_entries = []\n",
    "for file_name in file_names:\n",
    "    if '.txt' in file_name:\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(f'Parsed {count} of {len(file_names)}')\n",
    "        with open('/home/jupyter/form10k/' + file_name) as f:\n",
    "            f10_k = json.load(f)\n",
    "        embedding_entries.extend(create_text_embedding_entries(f10_k['item1'], f10_k['companyName'], f10_k['cusip']))\n",
    "len(embedding_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446e651-9134-4634-a527-4f2745c7d623",
   "metadata": {},
   "source": [
    "## Save 10-K Documents with Embeddings\n",
    "We will save these locally to use in graph loading, in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54ca3c83-fdc8-4d30-bacd-4c0b0328655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "edf = pd.DataFrame(embedding_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "971cd795-4097-415f-83d6-f4caa651218a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyName</th>\n",
       "      <th>cusip</th>\n",
       "      <th>seqId</th>\n",
       "      <th>contextId</th>\n",
       "      <th>textEmbedding</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>General motors co</td>\n",
       "      <td>369604103</td>\n",
       "      <td>0</td>\n",
       "      <td>General motors co0</td>\n",
       "      <td>[0.021373910829424858, -0.008970730938017368, ...</td>\n",
       "      <td>&gt;Item 1. Business \\nGeneral Motors Company (so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General motors co</td>\n",
       "      <td>369604103</td>\n",
       "      <td>1</td>\n",
       "      <td>General motors co1</td>\n",
       "      <td>[-0.033133506774902344, -0.012916910462081432,...</td>\n",
       "      <td>Our vision for the future is a world with zero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General motors co</td>\n",
       "      <td>369604103</td>\n",
       "      <td>2</td>\n",
       "      <td>General motors co2</td>\n",
       "      <td>[-0.04687150940299034, -0.022455034777522087, ...</td>\n",
       "      <td>In September 2021, we announced three new driv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General motors co</td>\n",
       "      <td>369604103</td>\n",
       "      <td>3</td>\n",
       "      <td>General motors co3</td>\n",
       "      <td>[-0.04158658906817436, -0.005550578236579895, ...</td>\n",
       "      <td>Ultium Charge 360 is also available to our fle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General motors co</td>\n",
       "      <td>369604103</td>\n",
       "      <td>4</td>\n",
       "      <td>General motors co4</td>\n",
       "      <td>[-0.037388477474451065, -0.007613219786435366,...</td>\n",
       "      <td>We offer OnStar and connected services to more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>11</td>\n",
       "      <td>ARK RESTAURANTS CORP11</td>\n",
       "      <td>[-0.0341801680624485, -0.02265389822423458, -0...</td>\n",
       "      <td>We have experienced aggressive competition for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>12</td>\n",
       "      <td>ARK RESTAURANTS CORP12</td>\n",
       "      <td>[-0.04283265769481659, -0.028135832399129868, ...</td>\n",
       "      <td>9\\nAlcoholic beverage control regulations requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>13</td>\n",
       "      <td>ARK RESTAURANTS CORP13</td>\n",
       "      <td>[-0.031862545758485794, -0.02629256248474121, ...</td>\n",
       "      <td>We are subject to “dram-shop” statutes in most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>14</td>\n",
       "      <td>ARK RESTAURANTS CORP14</td>\n",
       "      <td>[-0.02673407830297947, -0.02919837087392807, 0...</td>\n",
       "      <td>Our business is highly seasonal; however, our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>ARK RESTAURANTS CORP</td>\n",
       "      <td>00214Q104</td>\n",
       "      <td>15</td>\n",
       "      <td>ARK RESTAURANTS CORP15</td>\n",
       "      <td>[-0.048986222594976425, -0.033875517547130585,...</td>\n",
       "      <td>Our principal executive offices are located at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              companyName      cusip  seqId               contextId  \\\n",
       "0       General motors co  369604103      0      General motors co0   \n",
       "1       General motors co  369604103      1      General motors co1   \n",
       "2       General motors co  369604103      2      General motors co2   \n",
       "3       General motors co  369604103      3      General motors co3   \n",
       "4       General motors co  369604103      4      General motors co4   \n",
       "..                    ...        ...    ...                     ...   \n",
       "130  ARK RESTAURANTS CORP  00214Q104     11  ARK RESTAURANTS CORP11   \n",
       "131  ARK RESTAURANTS CORP  00214Q104     12  ARK RESTAURANTS CORP12   \n",
       "132  ARK RESTAURANTS CORP  00214Q104     13  ARK RESTAURANTS CORP13   \n",
       "133  ARK RESTAURANTS CORP  00214Q104     14  ARK RESTAURANTS CORP14   \n",
       "134  ARK RESTAURANTS CORP  00214Q104     15  ARK RESTAURANTS CORP15   \n",
       "\n",
       "                                         textEmbedding  \\\n",
       "0    [0.021373910829424858, -0.008970730938017368, ...   \n",
       "1    [-0.033133506774902344, -0.012916910462081432,...   \n",
       "2    [-0.04687150940299034, -0.022455034777522087, ...   \n",
       "3    [-0.04158658906817436, -0.005550578236579895, ...   \n",
       "4    [-0.037388477474451065, -0.007613219786435366,...   \n",
       "..                                                 ...   \n",
       "130  [-0.0341801680624485, -0.02265389822423458, -0...   \n",
       "131  [-0.04283265769481659, -0.028135832399129868, ...   \n",
       "132  [-0.031862545758485794, -0.02629256248474121, ...   \n",
       "133  [-0.02673407830297947, -0.02919837087392807, 0...   \n",
       "134  [-0.048986222594976425, -0.033875517547130585,...   \n",
       "\n",
       "                                                  text  \n",
       "0    >Item 1. Business \\nGeneral Motors Company (so...  \n",
       "1    Our vision for the future is a world with zero...  \n",
       "2    In September 2021, we announced three new driv...  \n",
       "3    Ultium Charge 360 is also available to our fle...  \n",
       "4    We offer OnStar and connected services to more...  \n",
       "..                                                 ...  \n",
       "130  We have experienced aggressive competition for...  \n",
       "131  9\\nAlcoholic beverage control regulations requ...  \n",
       "132  We are subject to “dram-shop” statutes in most...  \n",
       "133  Our business is highly seasonal; however, our ...  \n",
       "134  Our principal executive offices are located at...  \n",
       "\n",
       "[135 rows x 6 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e64c29-2139-4694-ba7f-751bbd8c9daf",
   "metadata": {},
   "source": [
    "Provide your Neo4j credentials.  We need the DB conection URL, the username (probably `neo4j`), and your password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58743d29-c611-4927-b61a-2b05721a9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# username is neo4j by default\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "\n",
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://6688b25b.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = '_kogrNk53u8oTk5be55kmit1kHGdhZj98yJlG-VYSR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "916b25a2-4dc8-454d-8fb6-06862b115ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f323e2e-7288-4887-b104-35be62dcae99",
   "metadata": {},
   "source": [
    "Remember to create indexes. We will be merging 10K documents by `companyName`. In a production setting, we would want to use a better identifier here (like we did with cusip for Company) However, this should suffice for our intents and purposes as we are just getting acquainted to learning about semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a57ef3d0-83f8-4d6d-9824-2f5206c36f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE INDEX company_name IF NOT EXISTS FOR (n:Company) ON (n.companyName)')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_document_id IF NOT EXISTS FOR (n:Document) REQUIRE (n.documentId) IS NODE KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96be481f-7be8-4e70-b441-98c9d629b56b",
   "metadata": {},
   "source": [
    "Due to the size of the documents we will want to transform the dataframe into a list of dict that we can chunk up and insert via parameterized query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0e68d63-b162-457b-8a60-aec01828aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_entries = edf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eecde179-7b86-46b8-a3dd-ae75b902472a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 of 135\n",
      "loaded 135 of 135\n"
     ]
    }
   ],
   "source": [
    "total = len(emb_entries)\n",
    "count = 0\n",
    "for d in chunks(emb_entries, 100):\n",
    "    gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH(c:Company {cusip:record.cusip})\n",
    "    MERGE(b:Document {documentId:record.contextId})\n",
    "    SET b.documentType = 'FORM_10K_ITEM1', b.seqId = record.seqId, b.textEmbedding = record.textEmbedding, b.text = record.text\n",
    "    MERGE(c)-[:HAS]->(b)\n",
    "    RETURN count(b) as cnt\n",
    "    ''', params = {'records':d})\n",
    "    count += len(d)\n",
    "    print(f'loaded {count} of {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75200e1-d4d3-4aaf-aae5-b989a99a855c",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45754ddd-c62c-4d4f-9e80-0648e99933db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(doc)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(doc)\n",
       "0         135"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check node count\n",
    "gds.run_cypher('MATCH(doc:Document) RETURN count(doc)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf4496-6783-4ec3-be91-2dad92bc477b",
   "metadata": {},
   "source": [
    "Note that we were only getting 10-K docs for a minority of companies. It should be fine for this, but in a more rigorous setting, you may want to try and pull more.  There are likely a few factors attributing to this. \n",
    "\n",
    "1. We used company names to search EDGAR which resulted in many misses and dups which were discarded. In a more rigorous setting, we would investigate other endpoints and use more parsing to extract EDGAR cik keys for exact matching companies when pulling forms.\n",
    "\n",
    "2. Company names are not consistent across form13 filings, so even if we successfully pull on one version of a company name, we may not be able to merge it into the graph via the one company name represented there. \n",
    "\n",
    "3. Not all companies in the dataset are obligated to file 10-Ks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1034a8eb-cd6d-4a09-913b-d1d5e97c1bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>numWithDocs</th>\n",
       "      <th>PercWithDocs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220</td>\n",
       "      <td>5</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total  numWithDocs  PercWithDocs\n",
       "0    220            5          2.27"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check count and percentage of companies with 10-K docs.  Note it is the minority\n",
    "gds.run_cypher('''\n",
    "MATCH(b:Company)\n",
    "WITH b, count{(b)-[:HAS]->(d:Document)} AS docCount\n",
    "WITH count(b) AS total, sum(toInteger(docCount > 0)) AS numWithDocs\n",
    "RETURN total, numWithDocs, round(100*toFloat(numWithDocs)/toFloat(total), 2) As PercWithDocs\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9c940-40cd-4fe9-8278-ef064e43933e",
   "metadata": {},
   "source": [
    "You might note that there are duplicate names.  For our purposes here, we will treat it as entity resolution, meaning that we treat companies with the same name as belonging to the same overarching entity for semantic search. In a more rigorous setting, we would need to disambiguate with other EDGAR keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da6b2835-2dfe-42ff-854f-c639eee67672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalCompanies</th>\n",
       "      <th>uniqueCompanyNames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   totalCompanies  uniqueCompanyNames\n",
       "0             220                 219"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show duplicates via HAS relationship\n",
    "gds.run_cypher('''\n",
    "MATCH(b:Company)\n",
    "RETURN count(b) AS totalCompanies, count(DISTINCT b.companyName) AS uniqueCompanyNames\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64854d9c-1776-451b-a3cb-60414d8116e3",
   "metadata": {},
   "source": [
    "## View Embeddings as Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb9e08-bbaf-494c-bf47-d9dc49944300",
   "metadata": {},
   "source": [
    "Vector embeddings generated by language models are nothing but numerical representation of words or sentences.  So, similar sentences will be located nearby.  The embeddings we generated earlier are higher dimensional ones.  To visualize them, we need to reduce the dimensionality.  Let's do that and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f690e19-1d53-47fe-974c-9fda3c5903ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def generate_chart(df, xcol, ycol, lbl = 'on', color = 'basic', title = '', tooltips = ['documentId'], label = ''):\n",
    "  chart = alt.Chart(df).mark_circle(size=30).encode(\n",
    "    x = alt.X(xcol,\n",
    "        scale=alt.Scale(zero = False),\n",
    "        axis=alt.Axis(labels = False, ticks = False, domain = False)\n",
    "    ),\n",
    "    y = alt.Y(ycol,\n",
    "        scale=alt.Scale(zero = False),\n",
    "        axis=alt.Axis(labels = False, ticks = False, domain = False)\n",
    "    ),\n",
    "    color= alt.value('#333293') if color == 'basic' else color,\n",
    "    tooltip=tooltips\n",
    "    )\n",
    "\n",
    "  if lbl == 'on':\n",
    "    text = chart.mark_text(align = 'left', baseline = 'middle', dx = 7, size = 5, color = 'black').encode(text = label, color = alt.value('black'))\n",
    "  else:\n",
    "    text = chart.mark_text(align = 'left', baseline = 'middle', dx = 10).encode()\n",
    "\n",
    "  result = (chart + text).configure(background=\"#FDF7F0\"\n",
    "        ).properties(\n",
    "        width = 800,\n",
    "        height = 500,\n",
    "        title = title\n",
    "       ).configure_legend(\n",
    "  orient = 'bottom', titleFontSize = 18, labelFontSize = 18)\n",
    "        \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e875832-4e4c-47d3-a535-e0b5fb177284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to return the principal components\n",
    "def get_pc(arr, n):\n",
    "  pca = PCA(n_components = n)\n",
    "  embeds_transform = pca.fit_transform(arr)\n",
    "  return embeds_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3aa76707-4150-4948-83b0-d454f086afe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyName</th>\n",
       "      <th>documentId</th>\n",
       "      <th>text</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARK INNOVATION ETF</td>\n",
       "      <td>ARK RESTAURANTS CORP0</td>\n",
       "      <td>&gt;Item 1.\\n \\nBusiness\\nCOVID-19 Pandemic and I...</td>\n",
       "      <td>[-0.042690303176641464, -0.02283802069723606, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARK INNOVATION ETF</td>\n",
       "      <td>ARK RESTAURANTS CORP1</td>\n",
       "      <td>Overview\\nWe are a New York corporation formed...</td>\n",
       "      <td>[-0.0489288792014122, -0.02295021153986454, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARK INNOVATION ETF</td>\n",
       "      <td>ARK RESTAURANTS CORP10</td>\n",
       "      <td>Competition\\nThe hospitality industry is highl...</td>\n",
       "      <td>[-0.04103657230734825, -0.037126582115888596, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARK INNOVATION ETF</td>\n",
       "      <td>ARK RESTAURANTS CORP11</td>\n",
       "      <td>We have experienced aggressive competition for...</td>\n",
       "      <td>[-0.0341801680624485, -0.02265389822423458, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARK INNOVATION ETF</td>\n",
       "      <td>ARK RESTAURANTS CORP12</td>\n",
       "      <td>9\\nAlcoholic beverage control regulations requ...</td>\n",
       "      <td>[-0.04283265769481659, -0.028135832399129868, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>VERIZON</td>\n",
       "      <td>Verizon Communications INC5</td>\n",
       "      <td>. We sell network access to mobile virtual net...</td>\n",
       "      <td>[0.016797110438346863, -0.04080076143145561, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>VERIZON</td>\n",
       "      <td>Verizon Communications INC6</td>\n",
       "      <td>Global Enterprise offers a broad portfolio of ...</td>\n",
       "      <td>[-0.0034878277219831944, -0.0191048514097929, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>VERIZON</td>\n",
       "      <td>Verizon Communications INC7</td>\n",
       "      <td>Public Sector and Other offers wireless produc...</td>\n",
       "      <td>[0.01294224988669157, -0.024389244616031647, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>VERIZON</td>\n",
       "      <td>Verizon Communications INC8</td>\n",
       "      <td>Local services\\n. We offer an array of local d...</td>\n",
       "      <td>[0.027821676805615425, -0.04315909743309021, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>VERIZON</td>\n",
       "      <td>Verizon Communications INC9</td>\n",
       "      <td>With respect to our wireless connectivity prod...</td>\n",
       "      <td>[0.004420581739395857, -0.034926533699035645, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            companyName                   documentId  \\\n",
       "0    ARK INNOVATION ETF        ARK RESTAURANTS CORP0   \n",
       "1    ARK INNOVATION ETF        ARK RESTAURANTS CORP1   \n",
       "2    ARK INNOVATION ETF       ARK RESTAURANTS CORP10   \n",
       "3    ARK INNOVATION ETF       ARK RESTAURANTS CORP11   \n",
       "4    ARK INNOVATION ETF       ARK RESTAURANTS CORP12   \n",
       "..                  ...                          ...   \n",
       "130             VERIZON  Verizon Communications INC5   \n",
       "131             VERIZON  Verizon Communications INC6   \n",
       "132             VERIZON  Verizon Communications INC7   \n",
       "133             VERIZON  Verizon Communications INC8   \n",
       "134             VERIZON  Verizon Communications INC9   \n",
       "\n",
       "                                                  text  \\\n",
       "0    >Item 1.\\n \\nBusiness\\nCOVID-19 Pandemic and I...   \n",
       "1    Overview\\nWe are a New York corporation formed...   \n",
       "2    Competition\\nThe hospitality industry is highl...   \n",
       "3    We have experienced aggressive competition for...   \n",
       "4    9\\nAlcoholic beverage control regulations requ...   \n",
       "..                                                 ...   \n",
       "130  . We sell network access to mobile virtual net...   \n",
       "131  Global Enterprise offers a broad portfolio of ...   \n",
       "132  Public Sector and Other offers wireless produc...   \n",
       "133  Local services\\n. We offer an array of local d...   \n",
       "134  With respect to our wireless connectivity prod...   \n",
       "\n",
       "                                                   emb  \n",
       "0    [-0.042690303176641464, -0.02283802069723606, ...  \n",
       "1    [-0.0489288792014122, -0.02295021153986454, 0....  \n",
       "2    [-0.04103657230734825, -0.037126582115888596, ...  \n",
       "3    [-0.0341801680624485, -0.02265389822423458, -0...  \n",
       "4    [-0.04283265769481659, -0.028135832399129868, ...  \n",
       "..                                                 ...  \n",
       "130  [0.016797110438346863, -0.04080076143145561, -...  \n",
       "131  [-0.0034878277219831944, -0.0191048514097929, ...  \n",
       "132  [0.01294224988669157, -0.024389244616031647, -...  \n",
       "133  [0.027821676805615425, -0.04315909743309021, -...  \n",
       "134  [0.004420581739395857, -0.034926533699035645, ...  \n",
       "\n",
       "[135 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = gds.run_cypher(\"MATCH (c:Company)-[:HAS]->(n:Document) RETURN c.companyName as companyName, n.documentId as documentId, n.text as text, n.textEmbedding as emb LIMIT 1000\")\n",
    "emb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99731d73-3648-43f1-b99d-79300f7df67d",
   "metadata": {},
   "source": [
    "## K-Means Clustering on the Embeddings\n",
    "Let's run the K-Means Clustering algorithm and view similar document chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2335c8-34bc-42ee-b093-ec3c5f718a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "embeds = np.array(emb_df['emb'].tolist())\n",
    "embeds_pc2 = get_pc(embeds, 2)\n",
    "\n",
    "df_clust = pd.concat([emb_df, pd.DataFrame(embeds_pc2)], axis = 1)\n",
    "n_clusters = 5\n",
    "\n",
    "kmeans_model = KMeans(n_clusters = n_clusters, n_init = 1, random_state = 0)\n",
    "classes = kmeans_model.fit_predict(embeds).tolist()\n",
    "df_clust['cluster'] = (list(map(str,classes)))\n",
    "\n",
    "df_clust.columns = df_clust.columns.astype(str)\n",
    "generate_chart(df_clust.iloc[:],'0', '1', lbl = 'off', color = 'cluster', title = 'K-Means Clustering with n Clusters', tooltips = ['documentId', 'text'], label = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d41a19-206d-4629-a969-bb74a021d2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
