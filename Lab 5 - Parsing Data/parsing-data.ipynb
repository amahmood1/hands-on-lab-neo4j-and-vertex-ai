{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ae6874-7585-4a0b-848c-965f639def41",
   "metadata": {},
   "source": [
    "# Parsing Data\n",
    "In this notebook, let's explore how to leverage generative AI to build and consume a knowledge graph in Neo4j.\n",
    "\n",
    "This notebook parses Form-13 data from SEC EDGAR. This is partially structured data, a mix of text and XML.  Instead of spending our time writing a bespoke parser to extract data from these files and load into Neo4j, we can prompt a Large Language Model (LLM) to do this for us automatically.  We will then also use the LLM to generate Cypher statements to load the extracted data into a Neo4j graph.\n",
    "\n",
    "## Setup\n",
    "First, let's install the libraries we're going to need for this lab and the following notebook dependent labs.  We'll also want to reboot the kernel once done.  To do that, go to the \"Kernel\" menu and click \"Restart Kernel and Clear All Outputs.\"  That will get rid of everything the install statements printed, leaving us with a cleaner notebook to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aad4e36-7048-4bbf-a71a-a5d1b24d0ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphdatascience\n",
      "  Obtaining dependency information for graphdatascience from https://files.pythonhosted.org/packages/56/a7/0d5a36acfaf551fa9abcb99c32cf3ff91401282200e3d2f01cc5c64c2190/graphdatascience-1.7-py3-none-any.whl.metadata\n",
      "  Downloading graphdatascience-1.7-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (1.9.1)\n",
      "Collecting neo4j<6.0,>=4.4.2 (from graphdatascience)\n",
      "  Downloading neo4j-5.13.0.tar.gz (192 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (2.0.3)\n",
      "Requirement already satisfied: pyarrow<13.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (12.0.1)\n",
      "Collecting textdistance<5.0,>=4.0 (from graphdatascience)\n",
      "  Obtaining dependency information for textdistance<5.0,>=4.0 from https://files.pythonhosted.org/packages/44/7b/13e25cbe91947672aeec2637b227398c3e97ec04ad0eaaac9125726ab0e7/textdistance-4.6.0-py3-none-any.whl.metadata\n",
      "  Downloading textdistance-4.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (4.7.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j<6.0,>=4.4.2->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Downloading graphdatascience-1.7-py3-none-any.whl (938 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.7/938.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading textdistance-4.6.0-py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: neo4j\n",
      "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neo4j: filename=neo4j-5.13.0-py3-none-any.whl size=265313 sha256=0dcd320cfe379dbde22df7bcf4b8424e52397fcaf1c10a44a9f5aa94d49620c5\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/7b/1d/b6/1be3a1e9de57bc832b7fcebbbf884186d8155bb6f1cc45be99\n",
      "Successfully built neo4j\n",
      "Installing collected packages: textdistance, neo4j, graphdatascience\n",
      "Successfully installed graphdatascience-1.7 neo4j-5.13.0 textdistance-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pydantic==1.10.11\n",
      "  Obtaining dependency information for pydantic==1.10.11 from https://files.pythonhosted.org/packages/b6/8e/7dd215f91528487535e7aa048e4092c20ecd0168df958e58809e2235cece/pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.10.11) (4.7.1)\n",
      "Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic\n",
      "Successfully installed pydantic-1.10.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain>=0.0.216\n",
      "  Obtaining dependency information for langchain>=0.0.216 from https://files.pythonhosted.org/packages/63/d1/84b1eabf897f2d964b682f55c4fc79e9c81e7367013c816f2f7f00020f5e/langchain-0.0.310-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.0.310-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (2.0.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (3.8.5)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain>=0.0.216)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/21/1f/1cff009cff64420572b9f75b70e4a054095719179a172297dfdd65843162/dataclasses_json-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.216)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.40 (from langchain>=0.0.216)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.40 from https://files.pythonhosted.org/packages/9b/6c/fd466f647634ef4a668ea109bf0892d7f78882ffe09500429081ed6dae4a/langsmith-0.0.43-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.43-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain>=0.0.216) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.216) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain>=0.0.216) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain>=0.0.216) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0->langchain>=0.0.216) (1.1.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.216) (2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.0.216) (4.7.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.216) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.216) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.216) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216) (23.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading langchain-0.0.310-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: mypy-extensions, marshmallow, jsonpatch, typing-inspect, langsmith, dataclasses-json, langchain\n",
      "\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts langchain and langchain-server are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.1 jsonpatch-1.33 langchain-0.0.310 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gradio\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/05/f2/360ca9546cffa45fee1df56864fdc2b6955de622e98435539490cd882a96/gradio-3.47.1-py3-none-any.whl.metadata\n",
      "  Downloading gradio-3.47.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Obtaining dependency information for altair<6.0,>=4.2.0 from https://files.pythonhosted.org/packages/17/16/b12fca347ff9d062e3c44ad9641d2ec50364570a059f3078ada3a5119d7a/altair-5.1.2-py3-none-any.whl.metadata\n",
      "  Downloading altair-5.1.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.101.1)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.6.0 (from gradio)\n",
      "  Obtaining dependency information for gradio-client==0.6.0 from https://files.pythonhosted.org/packages/9f/dd/db8dcc8521aa475a18a562929de0806819f8fa73ee8654d2cc22c836c3bd/gradio_client-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading gradio_client-0.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio)\n",
      "  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting huggingface-hub>=0.14.0 (from gradio)\n",
      "  Obtaining dependency information for huggingface-hub>=0.14.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.2)\n",
      "Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.23.5)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Obtaining dependency information for orjson~=3.0 from https://files.pythonhosted.org/packages/5d/2a/7719c0e6a812037d3de563f34fd7c854933b2bcade98ea5ab11230c0b735/orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.0.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (1.10.11)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.31.0)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.7.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.23.2)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.6.0->gradio) (2023.6.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (2023.7.22)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio)\n",
      "  Obtaining dependency information for httpcore<0.19.0,>=0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.19.0,>=0.18.0->httpx->gradio) (3.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.19.0,>=0.18.0->httpx->gradio) (1.1.3)\n",
      "Downloading gradio-3.47.1-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.6.0-py3-none-any.whl (298 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.1.2-py3-none-any.whl (516 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.2/516.2 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=b8c8513b26c84a9456b65d575c46c92ac453a3ef351653fdc47e3ca71f5800f6\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, aiofiles, huggingface-hub, httpcore, httpx, gradio-client, altair, gradio\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts gradio and upload_theme are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed aiofiles-23.2.1 altair-5.1.2 ffmpy-0.3.1 gradio-3.47.1 gradio-client-0.6.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.17.3 orjson-3.9.7 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 websockets-11.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting IProgress\n",
      "  Downloading IProgress-0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from IProgress) (1.16.0)\n",
      "Installing collected packages: IProgress\n",
      "Successfully installed IProgress-0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user graphdatascience\n",
    "%pip install --user \"pydantic==1.10.11\"\n",
    "%pip install --user \"langchain>=0.0.216\"\n",
    "%pip install --user gradio\n",
    "%pip install --user IProgress\n",
    "%pip install --user tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7c539-68b5-4126-990e-60c85b84fafa",
   "metadata": {},
   "source": [
    "Now restart the kernel. That will allow the Python evironment to import the new packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a385-8d3a-4c69-9817-ed14ba8e6511",
   "metadata": {},
   "source": [
    "## Prompt Definition\n",
    "We will extract knowledge adhering to the same schema we used previously.  To teach the LLM about the schema, we will use a series of prompts.  Each prompt is focused on only one task, extracting a specific entity:\n",
    "\n",
    "1. Manager Information\n",
    "2. Filing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeab081-6bc7-41a1-92a6-1f02b8adcf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr_info_tpl = \"\"\"From the text below, extract the following as json. Do not miss any of these information.\n",
    "* The tags mentioned below may or may not namespaced. So extract accordingly. Eg: <ns1:tag> is equal to <tag>\n",
    "* \"managerName\" - The name from the <name> tag under <filingManager> tag\n",
    "* \"street1\" - The manager's street1 address from the <com:street1> tag under <address> tag\n",
    "* \"street2\" - The manager's street2 address from the <com:street2> tag under <address> tag\n",
    "* \"city\" - The manager's city address from the <com:city> tag under <address> tag\n",
    "* \"stateOrCounty\" - The manager's stateOrCounty address from the <com:stateOrCountry> tag under <address> tag\n",
    "* \"zipCode\" - The manager's zipCode from the <com:zipCode> tag under <address> tag\n",
    "* \"reportCalendarOrQuarter\" - The reportCalendarOrQuarter from the <reportCalendarOrQuarter> tag under <address> tag\n",
    "* Just return me the JSON enclosed by 3 backticks. No other text in the response\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2483efd5-fedf-49b0-9f01-534bd7390968",
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_info_tpl = \"\"\"The text below contains a list of investments. Each instance of <infoTable> tag represents a unique investment. \n",
    "For each investment, please extract the below variables into json then combine into a list enclosed by 3 backticks. Please use the quoted names below while doing this\n",
    "* \"cusip\" - The cusip from the <cusip> tag under <infoTable> tag\n",
    "* \"companyName\" - The name under the <nameOfIssuer> tag.\n",
    "* \"value\" - The value from the <value> tag under <infoTable> tag. Return as a number. \n",
    "* \"shares\" - The sshPrnamt from the <sshPrnamt> tag under <infoTable> tag. Return as a number. \n",
    "* \"sshPrnamtType\" - The sshPrnamtType from the <sshPrnamtType> tag under <infoTable> tag\n",
    "* \"investmentDiscretion\" - The investmentDiscretion from the <investmentDiscretion> tag under <infoTable> tag\n",
    "* \"votingSole\" - The votingSole from the <votingSole> tag under <infoTable> tag\n",
    "* \"votingShared\" - The votingShared from the <votingShared> tag under <infoTable> tag\n",
    "* \"votingNone\" - The votingNone from the <votingNone> tag under <infoTable> tag\n",
    "\n",
    "Output format:\n",
    "* DO NOT output XML tags in the response. The output should be a valid JSON list enclosed by 3 backticks\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481717bb-6db2-4465-8667-b0c07da381d5",
   "metadata": {},
   "source": [
    "## Functions for Using LLMs\n",
    "Let's create some helper function to talk to the LLM with our prompt and text input. \n",
    "\n",
    "The [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models) describes the available foundation models.  We will use the text-bison base model. In some cases, there may be a need to fine-tune LLM models for KG creation. [Vertex AI provides an elegant way to fine-tune](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models) where the updated weights/model stay within your tenant and the base model is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e42db05-86be-4171-9ef3-538af376057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# Wrapper for calling language model\n",
    "def run_text_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    tuned_model_name: str = None,\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    if tuned_model_name is None:\n",
    "        model = TextGenerationModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca2aab-87a9-4628-9f17-6a8fd7d08153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for entity extraction and parsing\n",
    "def extract_entities_relationships(prompt, tuned_model_name=None):\n",
    "    try:\n",
    "        res = run_text_model(\"text-bison@001\", 0, 1024, 0.8, 1, prompt, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726ef3e9-e66f-45ed-be59-d386ce1db4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# splitting function for chunking up filing information to avoid hitting LLM token limits\n",
    "def split_filing_info(s, chunk_size=5):\n",
    "    pattern = '(</(\\w+:)?infoTable>)'\n",
    "    splitter = re.findall(pattern, s)[0][0]\n",
    "    _parts = s.split(splitter)\n",
    "    if len(_parts) > chunk_size:\n",
    "        chunks_of_list = np.array_split(_parts, len(_parts)/chunk_size) # max 5 filings per part\n",
    "        chunks_of_str = map(lambda x: splitter.join(x)+splitter, chunks_of_list)\n",
    "        l = list(chunks_of_str)\n",
    "        if len(l) > 0:\n",
    "            l[len(l)-1] = re.sub(f'{splitter}$', '', l[len(l)-1])\n",
    "        return l\n",
    "    else:\n",
    "        return [s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9dee8-c5dc-42ad-b05d-0f0d7e11d551",
   "metadata": {},
   "source": [
    "## Test Example for Parsing\n",
    "Let's start with one Form 13 file to see how we can parse it with Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57cc2dfa-4947-4afc-8b2a-6cd16338029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('neo4j-datasets')\n",
    "blob = bucket.blob('hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1026200_0001567619-22-000057.txt')\n",
    "\n",
    "inp_text = blob.download_as_string().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82c1ec-d49d-4444-b4fe-bc0be2777308",
   "metadata": {},
   "source": [
    "We can take a look at the file.  Note that it is an oddball mix of XML, delimeted and fixed spacing formatting that no standard parser could make sense of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33cda3eb-25c6-47cf-8d11-e1981766aeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEC-DOCUMENT>0001567619-22-000057.txt : 20220103\n",
      "<SEC-HEADER>0001567619-22-000057.hdr.sgml : 20220103\n",
      "<ACCEPTANCE-DATETIME>20220103171622\n",
      "ACCESSION NUMBER:\t\t0001567619-22-000057\n",
      "CONFORMED SUBMISSION TYPE:\t13F-HR\n",
      "PUBLIC DOCUMENT COUNT:\t\t2\n",
      "CONFORMED PERIOD OF REPORT:\t20211231\n",
      "FILED AS OF DATE:\t\t20220103\n",
      "DATE AS OF CHANGE:\t\t20220103\n",
      "EFFECTIVENESS DATE:\t\t20220103\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tPRIVATE ASSET MANAGEMENT INC\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001026200\n",
      "\t\tIRS NUMBER:\t\t\t\t330524568\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tCA\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t13F-HR\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t028-03581\n",
      "\t\tFILM NUMBER:\t\t22503075\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t5348 CARROLL CANYON RD\n",
      "\t\tSTREET 2:\t\t#200\n",
      "\t\tCITY:\t\t\tSAN DIEGO\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t92121\n",
      "\t\tBUSINESS PHONE:\t\t6197923800\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t5348 CARROLL CANYON RD\n",
      "\t\tSTREET 2:\t\t#200\n",
      "\t\tCITY:\t\t\tSAN DIEGO\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t92121\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>13F-HR\n",
      "<SEQUENCE>1\n",
      "<FILENAME>primary_doc.xml\n",
      "<TEXT>\n",
      "<XML>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<edgarSubmission xmlns=\"http://www.sec.gov/edgar/thirteenffiler\" xmlns:com=\"http://www.sec.gov/edgar/common\">\n",
      "  <headerData>\n",
      "    <submissionType>13F-HR</submissionType>\n",
      "    <filerInfo>\n",
      "      <liveTestFlag>LIVE</liveTestFlag>\n",
      "      <flags>\n",
      "        <confirmingCopyFlag>false</confirmingCopyFlag>\n",
      "        <returnCopyFlag>true</returnCopyFlag>\n",
      "        <overrideInternetFlag>false</overrideInternetFlag>\n",
      "      </flags>\n",
      "      <f\n"
     ]
    }
   ],
   "source": [
    "print(inp_text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41717f-0c6e-4642-b6fd-ca327ce474cf",
   "metadata": {},
   "source": [
    "We can split data into manager and filing info pieces using `<XML>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f23c86-3274-40fd-9c97-70337d8dd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = inp_text.split('<XML>')\n",
    "manager_info = contents[1].split('</XML>')[0].strip()\n",
    "filing_info = contents[2].split('</XML>')[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21286ffa-2ff2-40cd-8eee-8b4626f32c1f",
   "metadata": {},
   "source": [
    "## Parsing Manager Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522d7f7b-5b24-4d05-b0b6-a3d8a85dcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed35d7c5-ef70-419b-b9c3-69c34003a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the text below, extract the following as json. Do not miss any of these information.\n",
      "* The tags mentioned below may or may not namespaced. So extract accordingly. Eg: <ns1:tag> is equal to <tag>\n",
      "* \"managerName\" - The name from the <name> tag under <filingManager> tag\n",
      "* \"street1\" - The manager's street1 address from the <com:street1> tag under <address> tag\n",
      "* \"street2\" - The manager's street2 address from the <com:street2> tag under <address> tag\n",
      "* \"city\" - The manager's city address from the <com:city> tag under <address> tag\n",
      "* \"stateOrCounty\" - The manager's stateOrCounty address from the <com:stateOrCountry> tag under <address> tag\n",
      "* \"zipCode\" - The manager's zipCode from the <com:zipCode> tag under <address> tag\n",
      "* \"reportCalendarOrQuarter\" - The reportCalendarOrQuarter from the <reportCalendarOrQuarter> tag under <address> tag\n",
      "* Just return me the JSON enclosed by 3 backticks. No other text in the response\n",
      "\n",
      "Text:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<edgarSubmission xmlns=\"http://www.sec.gov/edgar/thirteenffiler\" xmlns:com=\"http://www.sec.gov/edgar/common\">\n",
      "  <headerData>\n",
      "    <submissionType>13F-HR</submissionType>\n",
      "    <filerInfo>\n",
      "      <liveTestFlag>LIVE</liveTestFlag>\n",
      "      <flags>\n",
      "        <confirmingCopyFlag>false</confirmingCopyFlag>\n",
      "        <returnCopyFlag>true</returnCopyFlag>\n",
      "        <overrideInternetFlag>false</overrideInternetFlag>\n",
      "      </flags>\n",
      "      <filer>\n",
      "        <credentials>\n",
      "          <cik>0001026200</cik>\n",
      "          <ccc>XXXXXXXX</ccc>\n",
      "        </credentials>\n",
      "      </filer>\n",
      "      <periodOfReport>12-31-2021</periodOfReport>\n",
      "    </filerInfo>\n",
      "  </headerData>\n",
      "  <formData>\n",
      "    <coverPage>\n",
      "      <reportCalendarOrQuarter>12-31-2021</reportCalendarOrQuarter>\n",
      "      <filingManager>\n",
      "        <name>PRIVATE ASSET MANAGEMENT INC</name>\n",
      "        <address>\n",
      "          <com:street1>5348 CARROLL CANYON RD</com:street1>\n",
      "          <com:street2>#200</com:street2>\n",
      "          <com:city>SAN DIEGO</com:city>\n",
      "          <com:stateOrCountry>CA</com:stateOrCountry>\n",
      "          <com:zipCode>92121</com:zipCode>\n",
      "        </address>\n",
      "      </filingManager>\n",
      "      <reportType>13F HOLDINGS REPORT</reportType>\n",
      "      <form13FFileNumber>028-03581</form13FFileNumber>\n",
      "      <provideInfoForInstruction5>N</provideInfoForInstruction5>\n",
      "    </coverPage>\n",
      "    <signatureBlock>\n",
      "      <name>Michael D. Berlin</name>\n",
      "      <title>General Counsel</title>\n",
      "      <phone>858-750-4209</phone>\n",
      "      <signature>/s/ Michael D. Berlin</signature>\n",
      "      <city>San Diego</city>\n",
      "      <stateOrCountry>CA</stateOrCountry>\n",
      "      <signatureDate>01-03-2022</signatureDate>\n",
      "    </signatureBlock>\n",
      "    <summaryPage>\n",
      "      <otherIncludedManagersCount>0</otherIncludedManagersCount>\n",
      "      <tableEntryTotal>149</tableEntryTotal>\n",
      "      <tableValueTotal>814509</tableValueTotal>\n",
      "      <isConfidentialOmitted>false</isConfidentialOmitted>\n",
      "    </summaryPage>\n",
      "  </formData>\n",
      "</edgarSubmission>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "\n",
    "prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b13100f-de4b-49f9-89f6-58e01c19727e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       " 'street1': '5348 CARROLL CANYON RD',\n",
       " 'street2': '#200',\n",
       " 'city': 'SAN DIEGO',\n",
       " 'stateOrCounty': 'CA',\n",
       " 'zipCode': '92121',\n",
       " 'reportCalendarOrQuarter': '12-31-2021'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Use LLM to parse out manager information\n",
    "manager_data = json.loads(extract_entities_relationships(prompt).split('```')[1].strip('json'))\n",
    "manager_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775186c0-d2bc-4711-88c4-0e61488ad2bd",
   "metadata": {},
   "source": [
    "## Parse Filing Information\n",
    "We will parse filing info in a similar manner to manager information. Because the filings include a list of many entries however, we will want to split the input into chunks so as not to exceed input or output token limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa1690a-012b-4056-8fcc-6e04e4bb26f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filing_info_chunks = split_filing_info(filing_info)\n",
    "len(filing_info_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125ed4c0-22b9-4fc8-83f0-a1ccc5eaa112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "\t{\n",
      "\t\t\"cusip\": \"747525103\",\n",
      "\t\t\"companyName\": \"Qualcomm Inc.\",\n",
      "\t\t\"value\": 93241,\n",
      "\t\t\"shares\": 509872,\n",
      "\t\t\"sshPrnamtType\": \"SH\",\n",
      "\t\t\"investmentDiscretion\": \"SOLE\",\n",
      "\t\t\"votingSole\": 0,\n",
      "\t\t\"votingShared\": 0,\n",
      "\t\t\"votingNone\": 509872\n",
      "\t},\n",
      "\t{\n",
      "\t\t\"cusip\": \"037833100\",\n",
      "\t\t\"companyName\": \"Apple Inc\",\n",
      "\t\t\"value\": 47051,\n",
      "\t\t\"shares\": 264971,\n",
      "\t\t\"sshPrnamtType\": \"SH\",\n",
      "\t\t\"investmentDiscretion\": \"SOLE\",\n",
      "\t\t\"votingSole\": 0,\n",
      "\t\t\"votingShared\": 0,\n",
      "\t\t\"votingNone\": 264971\n",
      "\t},\n",
      "\t{\n",
      "\t\t\"cusip\": \"594918104\",\n",
      "\t\t\"companyName\": \"Microsoft Corp\",\n",
      "\t\t\"value\": 37188,\n",
      "\t\t\"shares\": 110568,\n",
      "\t\t\"sshPrnamtType\": \"SH\",\n",
      "\t\t\"investmentDiscretion\": \"SOLE\",\n",
      "\t\t\"votingSole\": 0,\n",
      "\t\t\"votingShared\": 0,\n",
      "\t\t\"votingNone\": 110568\n",
      "\t},\n",
      "\t{\n",
      "\t\t\"cusip\": \"02079K107\",\n",
      "\t\t\"companyName\": \"Alphabet Inc C\",\n",
      "\t\t\"value\": 32478,\n",
      "\t\t\"shares\": 11224,\n",
      "\t\t\"sshPrnamtType\": \"SH\",\n",
      "\t\t\"investmentDiscretion\": \"SOLE\",\n",
      "\t\t\"votingSole\": 0,\n",
      "\t\t\"votingShared\": 0,\n",
      "\t\t\"votingNone\": 11224\n",
      "\t},\n",
      "\t{\n",
      "\t\t\"cusip\": \"437076102\",\n",
      "\t\t\"companyName\": \"The Home Depot Inc\",\n",
      "\t\t\"value\": 32006,\n",
      "\t\t\"shares\": 77119,\n",
      "\t\t\"sshPrnamtType\": \"SH\",\n",
      "\t\t\"investmentDiscretion\": \"SOLE\",\n",
      "\t\t\"votingSole\": 0,\n",
      "\t\t\"votingShared\": 0,\n",
      "\t\t\"votingNone\": 77119\n",
      "\t}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunks[0])\n",
    "response = extract_entities_relationships(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2560b-2e6c-4b96-abc3-7c96196d0eb5",
   "metadata": {},
   "source": [
    "## Test Example\n",
    "\n",
    "Let's walk through the steps to do this with just the 1 form above first, then we can move on to parsing and ingesting multiple form13s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a67de-b18f-4c65-a529-b890086663b8",
   "metadata": {},
   "source": [
    "To start we can run the LLM parsing over all the filing info from the form and then combine the resulting JSON into a list conducive for Neo4j loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827621eb-2f7d-4ef4-9f00-cddbc5e2cfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cusip': '747525103',\n",
       "  'companyName': 'Qualcomm Inc.',\n",
       "  'value': 93241,\n",
       "  'shares': 509872,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 509872,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '037833100',\n",
       "  'companyName': 'Apple Inc',\n",
       "  'value': 47051,\n",
       "  'shares': 264971,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 264971,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '594918104',\n",
       "  'companyName': 'Microsoft Corp',\n",
       "  'value': 37188,\n",
       "  'shares': 110568,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 110568,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '02079K107',\n",
       "  'companyName': 'Alphabet Inc C',\n",
       "  'value': 32478,\n",
       "  'shares': 11224,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 11224,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'},\n",
       " {'cusip': '437076102',\n",
       "  'companyName': 'The Home Depot Inc',\n",
       "  'value': 32006,\n",
       "  'shares': 77119,\n",
       "  'sshPrnamtType': 'SH',\n",
       "  'investmentDiscretion': 'SOLE',\n",
       "  'votingSole': 0,\n",
       "  'votingShared': 0,\n",
       "  'votingNone': 77119,\n",
       "  'managerName': 'PRIVATE ASSET MANAGEMENT INC',\n",
       "  'reportCalendarOrQuarter': '12-31-2021'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filings_list = []\n",
    "for filing_info_chunk in filing_info_chunks:\n",
    "    prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "    response = extract_entities_relationships(prompt)\n",
    "    if '```' in response:\n",
    "        response = response.split('```')[1].strip('json')\n",
    "    filings_list.extend(json.loads(response))\n",
    "\n",
    "for item in filings_list:\n",
    "    item['managerName'] = manager_data['managerName']\n",
    "    item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "filings_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f6b661-7d08-42fa-9dda-6ea9e8dfcffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b83ca6-c44b-4f43-9dcd-c88cf525a978",
   "metadata": {},
   "source": [
    "## Establish Neo4j Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fffd14-e6bb-4843-90e9-d1d4e7cef3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Username is neo4j by default\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "\n",
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://6688b25b.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = '_kogrNk53u8oTk5be55kmit1kHGdhZj98yJlG-VYSR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f38e2ef1-f5bd-4561-87e7-a014b8af2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6adf6-a299-47ca-a965-6b6cdec67f77",
   "metadata": {},
   "source": [
    "Before loading, we should create node key constraints for nodes.  This acts as a unique id and an index and is necessary for fast, efficient queries.  In general, if you notice ingestion is super slow (and getting slower) with Neo4j, double-check that you created indexes.  For this small sample, it won't matter, but it will undoubtedly impact as we ingest more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3084e75-da30-42c1-8ca4-10cf55e7c3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_manager IF NOT EXISTS FOR (n:Manager) REQUIRE (n.managerName) IS NODE KEY')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE (n.cusip) IS NODE KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814834e0-54f6-4e6f-9a6e-67a526cf3bf6",
   "metadata": {},
   "source": [
    "To merge the data, we can use parameterized Cypher queries.  Basically, we will send filings in batches (in this sample case, just one batch) for each node and relationship type and insert them as parameters in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feb25f22-39a0-4930-bf05-8adaa45c2f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_node_merge_count\n",
       "0                       149"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge company nodes\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (c:Company {cusip: record.cusip})\n",
    "SET c.companyName = record.companyName\n",
    "RETURN count(c) AS company_node_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635b9393-a2b6-4da6-94a4-cca6bcf515bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_node_merge_count\n",
       "0                         1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge manager node\n",
    "gds.run_cypher('''\n",
    "MERGE (m:Manager {managerName: $name})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'name':manager_data['managerName']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5394e270-45c3-44c9-96bb-6ea210dbb28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>owns_relationship_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   owns_relationship_merge_count\n",
       "0                            149"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge owns Relationship\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MATCH (m:Manager {managerName: record.managerName})\n",
    "MATCH (c:Company {cusip: record.cusip})\n",
    "MERGE(m)-[r:OWNS]->(c)\n",
    "SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,\n",
    "    r.value = record.value,\n",
    "    r.shares = record.shares\n",
    "RETURN count(r) AS owns_relationship_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ebde6-7e39-40fc-8457-4d39bb45efd1",
   "metadata": {},
   "source": [
    "## Ingest Multiple Form 13 Files\n",
    "We will make a pipeline using the methods above.  In this case we will take a two-step approach, first parse all the data, then chunk that data and ingest into Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0900b-0312-4213-9a3d-13e512226f93",
   "metadata": {},
   "source": [
    "For purposes of this lab we will just use a few form13 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29c1d593-dd40-4f08-ac0d-9c8e9ff201d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you have time to parse more files, you can uncomment these lines.\n",
    "sample_file_names = [\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1844571_0001844571-22-000001.txt',\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1875995_0001875995-22-000004.txt',\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-06_archives_edgar_data_1495703_0001495703-22-000002.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "568c27a4-d6bf-4e12-af6e-478e20fe5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for getting filing info\n",
    "def get_manager_and_filing_info(raw_txt):\n",
    "    contents = raw_txt.split('<XML>')\n",
    "    manager_info = contents[1].split('</XML>')[0].strip()\n",
    "    filing_info = contents[2].split('</XML>')[0].strip()\n",
    "    \n",
    "    return manager_info, filing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d46cc84-53b5-4088-a5aa-578f7eb5d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parsing 3 Form 13 Files ===\n",
      "--- parsing hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1844571_0001844571-22-000001.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1875995_0001875995-22-000004.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "--- parsing hands-on-lab/form13-raw/raw_2022-01-06_archives_edgar_data_1495703_0001495703-22-000002.txt ---\n",
      "getting file text from gcloud....\n",
      "getting file contents...\n",
      "Parsing submission and manager info...\n",
      "Parsing filing info...\n",
      "CPU times: user 828 ms, sys: 194 ms, total: 1.02 s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'=== Parsing {len(sample_file_names)} Form 13 Files ===')\n",
    "\n",
    "filings_list = []\n",
    "manager_list = []\n",
    "\n",
    "for file_name in sample_file_names:\n",
    "    \n",
    "    print(f'--- parsing {file_name} ---')\n",
    "    try:\n",
    "        # Get raw form13 file\n",
    "        print('getting file text from gcloud....')\n",
    "        blob = bucket.blob(file_name)\n",
    "        raw_text = blob.download_as_string().decode()\n",
    "\n",
    "        # Get raw manager and filing info from file\n",
    "        print('getting file contents...')\n",
    "        manager_info, filing_info = get_manager_and_filing_info(raw_text)\n",
    "\n",
    "        # Parse manager info into dict using LLM\n",
    "        print('Parsing submission and manager info...')\n",
    "        mng_prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "        mng_response = extract_entities_relationships(mng_prompt)\n",
    "        manager_data = json.loads(mng_response.replace('```', ''))\n",
    "        manager_list.append({'managerName': manager_data['managerName']})\n",
    "\n",
    "        # Parse filing info into list of dicts using LLM\n",
    "        print('Parsing filing info...')\n",
    "        tmp_filing_list = []\n",
    "        for filing_info_chunk in split_filing_info(filing_info):\n",
    "            filing_prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "            filing_response = extract_entities_relationships(filing_prompt)\n",
    "            if '```' in filing_response:\n",
    "                filing_response = filing_response.split('```')[1].strip('json')\n",
    "            tmp_filing_list.extend(json.loads(filing_response))\n",
    "        for item in tmp_filing_list: #Add information from manager_info to enable OWNS relationship loading\n",
    "            item['managerName'] = manager_data['managerName']\n",
    "            item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "        filings_list.extend(tmp_filing_list)\n",
    "    except Exception as e:\n",
    "        print(filing_response)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b656917-8b98-4b94-a905-fcd4a4a5f8bd",
   "metadata": {},
   "source": [
    "Now we can merge the mananger nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df9206bb-d619-4649-99cf-f5939461555a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_node_merge_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_node_merge_count\n",
       "0                         3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge manager nodes\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (m:Manager {managerName: record.managerName})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'records':manager_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c27c4-4e48-4a08-8aba-26a3874e7f3e",
   "metadata": {},
   "source": [
    "For filings lets check ther length of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c31f300-1c53-4795-ae1c-7228f240ddf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ede58-e686-42c8-90f7-fd8919a17f36",
   "metadata": {},
   "source": [
    "While we should not need chunking for this example, below is an example of how to chunk up a parameterized function for loading in case you need to scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ceb716e1-c71c-4992-b654-016f9496f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the dataset gets bigger we will want to chunk up the filings we send to Neo4j\n",
    "def chunks(xs, n=10_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2216015-d8ca-475d-8240-a5717081c8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   company_node_merge_count\n",
      "0                       140\n"
     ]
    }
   ],
   "source": [
    "# Merge company nodes\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MERGE (c:Company {cusip: record.cusip})\n",
    "    SET c.companyName = record.companyName\n",
    "    RETURN count(c) AS company_node_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "750215eb-ec16-4b42-909f-cf763b9955b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   owns_relationship_merge_count\n",
      "0                            140\n"
     ]
    }
   ],
   "source": [
    "# Merge owns Relationships\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH (m:Manager {managerName: record.managerName})\n",
    "    MATCH (c:Company {cusip: record.cusip})\n",
    "    MERGE(m)-[r:OWNS]->(c)\n",
    "    SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,\n",
    "        r.value = record.value,\n",
    "        r.shares = record.shares\n",
    "    RETURN count(r) AS owns_relationship_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88924cd-b43a-42ca-b975-87f1362987fc",
   "metadata": {},
   "source": [
    "This type of workflow can be applied to other unstructured data to parse entities and relationships with language models and load them into a Neo4j knowledge graph. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
