{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neo4j-partners/hands-on-lab-neo4j-and-vertex-ai/blob/main/Lab%206%20-%20Vertex%20AI/vertex_ai_embedding.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKipBL0kWY7w"
      },
      "source": [
        "# Install additional Packages\n",
        "First off, you'll also need to install a few packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDipS8p-27qg"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet google-cloud-storage\n",
        "!pip install --quiet google.cloud.aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXAh7fVt9Ou"
      },
      "source": [
        "# Restart the kernel\n",
        "After you install the additional packages, you need to restart the notebook kernel so it can find the packages.  When you run this, you may get a notification that the kernel crashed.  You can disregard that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySSyV4T_3dQB"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kGD4H-TqjwZ"
      },
      "source": [
        "# Split the Data\n",
        "Now let's grab the data set and split it into a training and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aemWQTWqjwZ"
      },
      "outputs": [],
      "source": [
        "# todo - wget from bucket in lab 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nbz-IKcqjwa"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "df = pandas.read_csv('form13.csv')\n",
        "\n",
        "train = df.loc[df['reportCalendarOrQuarter'] == '03-31-2021']\n",
        "train = train.append(df.loc[df['reportCalendarOrQuarter'] == '06-30-2021'])\n",
        "train.to_csv('train.csv', index=False)\n",
        "\n",
        "test = df.loc[df['reportCalendarOrQuarter'] == '09-30-2021']\n",
        "test.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6tjQDbgf2S"
      },
      "source": [
        "# Authenticate your Google Cloud account\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-FC4GI1H3jx"
      },
      "outputs": [],
      "source": [
        "# Edit these variables!\n",
        "PROJECT_ID = \"YOUR-PROJECT-ID\"\n",
        "STORAGE_BUCKET = \"YOUR-BUCKET-NAME\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XoT1nT_JlYx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can leave these defaults\n",
        "REGION = \"us-central1\"\n",
        "STORAGE_PATH = \"embedding\""
      ],
      "metadata": {
        "id": "HrHzI2PKrQC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HucMnpmVgfmX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUU7z4FjJS90"
      },
      "source": [
        "# Upload to a GCP Cloud Storage Bucket\n",
        "\n",
        "To get the data into Vertex AI, we must first put it in a bucket as a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3nbLg1cKJpJ"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "client = storage.Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dAkAU5ALnUo"
      },
      "outputs": [],
      "source": [
        "bucket = client.bucket(STORAGE_BUCKET)\n",
        "client.create_bucket(bucket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTo7-_oJL_dZ"
      },
      "outputs": [],
      "source": [
        "# Upload our files to that bucket\n",
        "for filename in ['train.csv', 'test.csv']:\n",
        "    upload_path = os.path.join(STORAGE_PATH, filename)\n",
        "    blob = bucket.blob(upload_path)\n",
        "    blob.upload_from_filename(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArK3cfKsdT1x"
      },
      "source": [
        "# Train a Model on GCP\n",
        "We'll use the engineered features to train an AutoML Tables model, then deploy it to an endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGjrD-k3dsCN"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "dataset = aiplatform.TabularDataset.create(\n",
        "    display_name=\"form13raw\",\n",
        "    gcs_source=os.path.join(\"gs://\", STORAGE_BUCKET, STORAGE_PATH, 'train.csv'),\n",
        ")\n",
        "dataset.wait()\n",
        "\n",
        "print(f'\\tDataset: \"{dataset.display_name}\"')\n",
        "print(f'\\tname: \"{dataset.resource_name}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaSPuk31N2xS"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.AutoMLTabularTrainingJob(\n",
        "    display_name='train-form13embedding-automl-1',\n",
        "    optimization_prediction_type='classification'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqf44y_G8vi1"
      },
      "outputs": [],
      "source": [
        "model = job.run(\n",
        "    dataset=dataset,\n",
        "    target_column='target',\n",
        "    training_fraction_split=0.8,\n",
        "    validation_fraction_split=0.1,\n",
        "    test_fraction_split=0.1,\n",
        "    model_display_name=\"form13embedding\",\n",
        "    disable_early_stopping=False,\n",
        "    budget_milli_node_hours=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This job will run for an hour.  That's the minimum time for an AutoML job.  We're going to move on to the next notebook.  You can check on the job later in the [Google Cloud Console](https://console.cloud.google.com/) to see the results.  There's a link to the specific job in the output of the cell above."
      ],
      "metadata": {
        "id": "u-d3_txVr39w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional Exercises\n",
        "Optional work follows.  In this, we deploy our model, create a feature store and then use that.  This is a walkthrough of how we might operationalize the model we created above."
      ],
      "metadata": {
        "id": "Z4cX4D3Ir5rL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoVThi28VO_R"
      },
      "outputs": [],
      "source": [
        "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NnDaATyWY7z"
      },
      "source": [
        "## Loading Data into GCP Feature Store\n",
        "In this section, we'll take our dataframe with newly engineered features and load that into GCP feature store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0DcYzPkRrzj"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1 import FeaturestoreServiceClient\n",
        "\n",
        "api_endpoint = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
        "fs_client = FeaturestoreServiceClient(client_options={\"api_endpoint\": api_endpoint})\n",
        "\n",
        "resource_path = fs_client.common_location_path(PROJECT_ID, REGION)\n",
        "fs_path = fs_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n",
        "entity_path = fs_client.entity_type_path(\n",
        "    PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMN4Ue2hjdL3"
      },
      "source": [
        "First, let's check if the Feature Store already exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQknjQFsVNC"
      },
      "outputs": [],
      "source": [
        "from grpc import StatusCode\n",
        "\n",
        "\n",
        "def check_has_resource(callable):\n",
        "    has_resource = False\n",
        "    try:\n",
        "        callable()\n",
        "        has_resource = True\n",
        "    except Exception as e:\n",
        "        if (\n",
        "            not hasattr(e, \"grpc_status_code\")\n",
        "            or e.grpc_status_code != StatusCode.NOT_FOUND\n",
        "        ):\n",
        "            raise e\n",
        "    return has_resource"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTVIsom6eejQ"
      },
      "outputs": [],
      "source": [
        "feature_store_exists = check_has_resource(\n",
        "    lambda: fs_client.get_featurestore(name=fs_path)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caTWbgeChd_x"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\n",
        "from google.cloud.aiplatform_v1.types import feature as feature_pb2\n",
        "from google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\n",
        "from google.cloud.aiplatform_v1.types import \\\n",
        "    featurestore_service as featurestore_service_pb2\n",
        "from google.cloud.aiplatform_v1.types import io as io_pb2\n",
        "\n",
        "if not feature_store_exists:\n",
        "    create_lro = fs_client.create_featurestore(\n",
        "        featurestore_service_pb2.CreateFeaturestoreRequest(\n",
        "            parent=resource_path,\n",
        "            featurestore_id=FEATURESTORE_ID,\n",
        "            featurestore=featurestore_pb2.Featurestore(\n",
        "                online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n",
        "                    fixed_node_count=1\n",
        "                ),\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(create_lro.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1JRwvvYJMBy"
      },
      "outputs": [],
      "source": [
        "entity_type_exists = check_has_resource(\n",
        "    lambda: fs_client.get_entity_type(name=entity_path)\n",
        ")\n",
        "\n",
        "if not entity_type_exists:\n",
        "    users_entity_type_lro = fs_client.create_entity_type(\n",
        "        featurestore_service_pb2.CreateEntityTypeRequest(\n",
        "            parent=fs_path,\n",
        "            entity_type_id=ENTITY_NAME,\n",
        "            entity_type=entity_type_pb2.EntityType(\n",
        "                description=\"Main entity type\",\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "    print(users_entity_type_lro.result())\n",
        "\n",
        "    feature_requests = [\n",
        "        featurestore_service_pb2.CreateFeatureRequest(\n",
        "            feature=feature_pb2.Feature(\n",
        "                value_type=feature_pb2.Feature.ValueType.DOUBLE,\n",
        "                description=\"Embedding {} from Neo4j\".format(i),\n",
        "            ),\n",
        "            feature_id=\"embedding_{}\".format(i),\n",
        "        )\n",
        "        for i in range(EMBEDDING_DIMENSION)\n",
        "    ]\n",
        "    create_features_lro = fs_client.batch_create_features(\n",
        "        parent=entity_path,\n",
        "        requests=feature_requests,\n",
        "    )\n",
        "    print(create_features_lro.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz78rmNrwK0V"
      },
      "outputs": [],
      "source": [
        "feature_specs = [\n",
        "    featurestore_service_pb2.ImportFeatureValuesRequest.FeatureSpec(\n",
        "        id=\"embedding_{}\".format(i)\n",
        "    )\n",
        "    for i in range(EMBEDDING_DIMENSION)\n",
        "]\n",
        "\n",
        "from google.protobuf.timestamp_pb2 import Timestamp\n",
        "\n",
        "feature_time = Timestamp()\n",
        "feature_time.GetCurrentTime()\n",
        "feature_time.nanos = 0\n",
        "\n",
        "import_request = fs_client.import_feature_values(\n",
        "    featurestore_service_pb2.ImportFeatureValuesRequest(\n",
        "        entity_type=entity_path,\n",
        "        csv_source=io_pb2.CsvSource(\n",
        "            gcs_source=io_pb2.GcsSource(\n",
        "                uris=[\n",
        "                    os.path.join(\n",
        "                        \"gs://\", STORAGE_BUCKET, STORAGE_PATH, FEATURES_FILENAME\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "        ),\n",
        "        entity_id_field=\"nodeId\",\n",
        "        feature_specs=feature_specs,\n",
        "        worker_count=1,\n",
        "        feature_time=feature_time,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(import_request.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOeih_WxWhSx"
      },
      "source": [
        "## Sending a prediction using features from the feature store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFr8zWyiWxOa"
      },
      "outputs": [],
      "source": [
        "from google.cloud.aiplatform_v1 import FeaturestoreOnlineServingServiceClient\n",
        "\n",
        "data_client = FeaturestoreOnlineServingServiceClient(\n",
        "    client_options={\"api_endpoint\": api_endpoint}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnVC3BHmWylQ"
      },
      "outputs": [],
      "source": [
        "# Retrieve Neo4j embeddings from feature store\n",
        "from google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n",
        "from google.cloud.aiplatform_v1.types import \\\n",
        "    featurestore_online_service as featurestore_online_service_pb2\n",
        "\n",
        "feature_selector = FeatureSelector(\n",
        "    id_matcher=IdMatcher(\n",
        "        ids=[\"embedding_{}\".format(i) for i in range(EMBEDDING_DIMENSION)]\n",
        "    )\n",
        ")\n",
        "\n",
        "fs_features = data_client.read_feature_values(\n",
        "    featurestore_online_service_pb2.ReadFeatureValuesRequest(\n",
        "        entity_type=entity_path,\n",
        "        entity_id=\"5\",\n",
        "        feature_selector=feature_selector,\n",
        "    )\n",
        ")\n",
        "\n",
        "saved_embeddings = dict(\n",
        "    zip(\n",
        "        (fd.id for fd in fs_features.header.feature_descriptors),\n",
        "        (str(d.value.double_value) for d in fs_features.entity_view.data),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgW3Ks0SihdN"
      },
      "outputs": [],
      "source": [
        "# Combine with other features. These might be sourced per transaction\n",
        "all_features = {\"num_transactions\": \"80\", \"total_dollar_amnt\": \"7484459.618641878\"}\n",
        "\n",
        "all_features.update(saved_embeddings)\n",
        "\n",
        "instances = [{key: str(value) for key, value in all_features.items()}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnK_FJeIi--4"
      },
      "outputs": [],
      "source": [
        "# Send a prediction\n",
        "endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU79nGz2gv_M"
      },
      "source": [
        "# Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es9wPH3UVbP-"
      },
      "outputs": [],
      "source": [
        "#Delete the feature store and turn down the endpoint\n",
        "fs_client.delete_featurestore(\n",
        "    request=featurestore_service_pb2.DeleteFeaturestoreRequest(\n",
        "        name=fs_client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n",
        "        force=True,\n",
        "    )\n",
        ").result()\n",
        "\n",
        "endpoint.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vertex_ai_embedding.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
